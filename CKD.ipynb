{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3Yj7HAqbeh4y"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report , confusion_matrix\n",
        "import joblib\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
        "from sklearn.model_selection import learning_curve, validation_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from google.colab import files\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset,Dataset\n",
        "from sklearn.model_selection import learning_curve as skl_learning_curve\n",
        "from torch.nn import functional as F\n",
        "import pickle\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/40K_dataset combined_and_shuffled.csv')\n",
        "df.head()\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "import pandas as pd\n",
        "\n",
        "# Define your file paths and target column names\n",
        "files_info = [\n",
        "    {\"file\": \"/content/imputed data.csv\", \"target_col\": \"stage\"},\n",
        "    {\"file\": \"/content/egfr 400.csv\", \"target_col\": \"ckd_stage\"},\n",
        "    {\"file\": \"/content/CKD_dataset_AbuDhabi (1).csv\", \"target_col\": \"CKD_stage\"},\n",
        "    {\"file\": \"/content/40K_dataset - 40K_dataset (1).csv\", \"target_col\": \"stage\"}\n",
        "]\n",
        "\n",
        "# Count class samples for each file\n",
        "for info in files_info:\n",
        "    df = pd.read_csv(info[\"file\"])\n",
        "    print(f\"\\nClass counts for {info['file']} (target column: {info['target_col']}):\")\n",
        "    print(df[info[\"target_col\"]].value_counts())\n"
      ],
      "metadata": {
        "id": "NtX8VHoleoO8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "c81ca76d-e1c9-4ddd-fc6b-274da710a07b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/40K_dataset combined_and_shuffled.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-42e9505e616c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/40K_dataset combined_and_shuffled.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dataset shape: {df.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/40K_dataset combined_and_shuffled.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# For cleaner plots\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.rcParams.update({'figure.max_open_warning': 0})  # avoid too many open figure warnings\n",
        "\n",
        "# List of file paths\n",
        "file_paths = [\"/content/imputed data.csv\", \"/content/egfr 400.csv\", \"/content/CKD_dataset_AbuDhabi (1).csv\", \"/content/40K_dataset - 40K_dataset (1).csv\"]\n",
        "\n",
        "# Function to plot feature distributions\n",
        "def plot_feature_distributions(df, file_name):\n",
        "    print(f\"\\n📊 Feature distributions for {file_name}\")\n",
        "    for column in df.columns:\n",
        "        plt.figure(figsize=(6, 4))\n",
        "        unique_vals = df[column].nunique()\n",
        "        dtype = df[column].dtype\n",
        "\n",
        "        # Categorical features\n",
        "        if dtype == 'object' or unique_vals <= 10:\n",
        "            value_counts = df[column].value_counts(dropna=False)\n",
        "\n",
        "            if unique_vals <= 5:\n",
        "                # Pie chart for very few categories\n",
        "                value_counts.plot.pie(autopct='%1.1f%%', startangle=90)\n",
        "                plt.ylabel('')\n",
        "                plt.title(f\"{column} - Pie Chart\")\n",
        "            else:\n",
        "                # Bar plot for more categories\n",
        "                sns.barplot(x=value_counts.index.astype(str), y=value_counts.values)\n",
        "                plt.title(f\"{column} - Bar Plot\")\n",
        "                plt.xticks(rotation=45, ha='right')\n",
        "                plt.ylabel('Count')\n",
        "                plt.xlabel(column)\n",
        "\n",
        "        # Numerical features\n",
        "        else:\n",
        "            sns.histplot(df[column].dropna(), kde=True, bins=30)\n",
        "            plt.title(f\"{column} - Histogram\")\n",
        "            plt.xlabel(column)\n",
        "            plt.ylabel('Frequency')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Read and visualize each file\n",
        "for path in file_paths:\n",
        "    try:\n",
        "        df = pd.read_csv(path)\n",
        "        plot_feature_distributions(df, path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {path}: {e}\")\n"
      ],
      "metadata": {
        "id": "L4N0BAMLe216",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46f0225b-0e00-4169-91d6-8b39d2b2effb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/imputed data.csv: [Errno 2] No such file or directory: '/content/imputed data.csv'\n",
            "Error processing /content/egfr 400.csv: [Errno 2] No such file or directory: '/content/egfr 400.csv'\n",
            "Error processing /content/CKD_dataset_AbuDhabi (1).csv: [Errno 2] No such file or directory: '/content/CKD_dataset_AbuDhabi (1).csv'\n",
            "Error processing /content/40K_dataset - 40K_dataset (1).csv: [Errno 2] No such file or directory: '/content/40K_dataset - 40K_dataset (1).csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# List of file paths\n",
        "file_paths = [\"/content/imputed data.csv\", \"/content/egfr 400.csv\", \"/content/CKD_dataset_AbuDhabi (1).csv\", \"/content/40K_dataset - 40K_dataset (1).csv\"]\n",
        "\n",
        "# Function to calculate and print missing value percentages\n",
        "def show_missing_percentages(df, file_name):\n",
        "    print(f\"\\n📋 Missing Value Percentage in {file_name}:\")\n",
        "    missing_percent = df.isnull().mean() * 100\n",
        "    missing_percent = missing_percent[missing_percent > 0].sort_values(ascending=False)\n",
        "    if missing_percent.empty:\n",
        "        print(\"✅ No missing values.\")\n",
        "    else:\n",
        "        print(missing_percent.round(2).to_string())\n",
        "\n",
        "# Iterate through each file\n",
        "for path in file_paths:\n",
        "    try:\n",
        "        df = pd.read_csv(path)\n",
        "        show_missing_percentages(df, path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {path}: {e}\")\n"
      ],
      "metadata": {
        "id": "trmu5Eige5DC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unique values\n",
        "def get_unique_values(dataframe, max_display=20):\n",
        "    for column in dataframe.columns:\n",
        "        unique_vals = dataframe[column].unique()\n",
        "        print(f\"\\n{column}:\")\n",
        "        print(f\"Number of unique values: {len(unique_vals)}\")\n",
        "        if len(unique_vals) <= max_display:\n",
        "            print(f\"Values: {unique_vals}\")\n",
        "        else:\n",
        "            print(f\"Too many values to display ({len(unique_vals)} unique values)\")\n",
        "\n",
        "get_unique_values(df)"
      ],
      "metadata": {
        "id": "P25oLx6Ce85D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Quality Analysis\n",
        "\n",
        "# Summary statistics for numerical columns\n",
        "print(\"\\nSummary Statistics for Numerical Columns:\")\n",
        "numerical_summary = df.describe()\n",
        "print(numerical_summary)\n",
        "\n",
        "# outliers in numerical columns (IQR method)\n",
        "print(\"\\n Outliers in Numerical Columns:\")\n",
        "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "for col in numerical_cols:\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    outliers_count = df[(df[col] < lower_bound) | (df[col] > upper_bound)].shape[0]\n",
        "    outliers_percentage = (outliers_count / df.shape[0]) * 100\n",
        "\n",
        "    print(f\"{col}: {outliers_count} outliers ({outliers_percentage:.2f}% of data)\")\n",
        "\n",
        "# counts for categorical columns\n",
        "print(\"\\nValue Counts for Categorical Columns (Top 5):\")\n",
        "categorical_cols = df.select_dtypes(include=['object', 'category', 'bool']).columns\n",
        "\n",
        "for col in categorical_cols:\n",
        "    print(f\"\\n{col}:\")\n",
        "    print(df[col].value_counts().head())\n",
        "\n",
        "# imbalanced binary features\n",
        "print(\"\\nImbalance Check for Binary Features:\")\n",
        "for col in df.columns:\n",
        "    if df[col].nunique() == 2:\n",
        "        value_counts = df[col].value_counts(normalize=True) * 100\n",
        "        print(f\"\\n{col}:\")\n",
        "        for value, percentage in value_counts.items():\n",
        "            print(f\"  {value}: {percentage:.2f}%\")"
      ],
      "metadata": {
        "id": "wS8hr22Ze_yK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Value Analysis\n",
        "\n",
        "# Count of missing values per column\n",
        "missing_values = df.isnull().sum()\n",
        "missing_percentage = (missing_values / len(df)) * 100\n",
        "\n",
        "missing_info = pd.DataFrame({\n",
        "    'Missing Values': missing_values,\n",
        "    'Percentage (%)': missing_percentage\n",
        "})\n",
        "missing_info = missing_info.sort_values('Missing Values', ascending=False)\n",
        "\n",
        "print(\"Missing Values Analysis:\")\n",
        "print(missing_info)\n",
        "\n",
        "print(\"\\nColumns with Missing Values:\")\n",
        "columns_with_missing = missing_info[missing_info['Missing Values'] > 0]\n",
        "if len(columns_with_missing) > 0:\n",
        "    print(columns_with_missing)\n",
        "else:\n",
        "    print(\"No missing values found in the dataset!\")\n",
        "\n",
        "print(\"\\nMissing Values Pattern (sample of 10 rows):\")\n",
        "if missing_values.sum() > 0:\n",
        "    # Get columns with missing values\n",
        "    cols_with_missing = [col for col in df.columns if df[col].isnull().any()]\n",
        "\n",
        "    sample_rows = df[df[cols_with_missing].isnull().any(axis=1)].sample(min(10, df[df[cols_with_missing].isnull().any(axis=1)].shape[0]))\n",
        "\n",
        "    pattern = sample_rows[cols_with_missing].isnull()\n",
        "\n",
        "\n",
        "    pattern_display = pattern.replace({True: 'Missing', False: 'Present'})\n",
        "    print(pattern_display)\n",
        "else:\n",
        "    print(\"No missing values to visualize!\")\n",
        "\n",
        "# total missing values\n",
        "total_missing = missing_values.sum()\n",
        "total_cells = df.size\n",
        "missing_percentage_total = (total_missing / total_cells) * 100\n",
        "\n",
        "print(f\"\\nTotal Missing Values: {total_missing} out of {total_cells} cells ({missing_percentage_total:.2f}%)\")"
      ],
      "metadata": {
        "id": "atGKRZcZfD0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Duplicate Check\n",
        "\n",
        "duplicate_rows = df.duplicated().sum()\n",
        "duplicate_percentage = (duplicate_rows / len(df)) * 100\n",
        "\n",
        "print(f\"Number of duplicate rows: {duplicate_rows} ({duplicate_percentage:.2f}% of the dataset)\")\n",
        "\n",
        "if duplicate_rows > 0:\n",
        "    print(\"\\nFirst 5 duplicate rows:\")\n",
        "    print(df[df.duplicated(keep='first')].head())\n",
        "\n",
        "    print(\"\\nChecking for potential ID columns or unique identifiers:\")\n",
        "    for col in df.columns:\n",
        "        unique_count = df[col].nunique()\n",
        "        unique_percentage = (unique_count / len(df)) * 100\n",
        "        if unique_count == len(df):\n",
        "            print(f\"Column '{col}' has all unique values (potential ID column)\")\n",
        "        elif unique_percentage > 95:\n",
        "            print(f\"Column '{col}' has {unique_count} unique values ({unique_percentage:.2f}% of rows)\")\n",
        "\n",
        "    print(\"\\nChecking for potential duplicate records based on key features:\")\n",
        "\n",
        "\n",
        "    demographic_cols = [col for col in df.columns if col in ['age', 'gender', 'ethnicity', 'BMI']]\n",
        "    if demographic_cols:\n",
        "        demographic_dupes = df.duplicated(subset=demographic_cols).sum()\n",
        "        print(f\"Duplicate records based on demographic features: {demographic_dupes}\")\n",
        "\n",
        "\n",
        "    medical_cols = [col for col in df.columns if col in ['diastole_pressure', 'creatinine', 'hemoglobin', 'egfr']]\n",
        "    if medical_cols:\n",
        "        medical_dupes = df.duplicated(subset=medical_cols).sum()\n",
        "        print(f\"Duplicate records based on key medical indicators: {medical_dupes}\")\n",
        "else:\n",
        "    print(\"No duplicate rows found in the dataset.\")"
      ],
      "metadata": {
        "id": "xmo8BQ2CfIUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Type Verification\n",
        "\n",
        "\n",
        "print(\"Current Data Types:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "print(\"\\n Data Type Issues:\")\n",
        "\n",
        "for col in df.columns:\n",
        "    if df[col].dtype == 'object':\n",
        "\n",
        "        try:\n",
        "            pd.to_numeric(df[col])\n",
        "            print(f\"Column '{col}' is stored as object but contains numeric values\")\n",
        "        except:\n",
        "\n",
        "            try:\n",
        "                pd.to_datetime(df[col])\n",
        "                print(f\"Column '{col}' is stored as object but contains date values\")\n",
        "            except:\n",
        "\n",
        "                if df[col].nunique() <= 2 and set(df[col].dropna().unique()).issubset({'True', 'False', 'true', 'false', 'Yes', 'No', 'yes', 'no', 'Y', 'N', 'y', 'n', '1', '0', 1, 0}):\n",
        "                    print(f\"Column '{col}' is stored as object but might be boolean\")\n",
        "\n",
        "# Check for inconsistent values in categorical columns\n",
        "print(\"\\nInconsistent Values in Categorical Columns:\")\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "for col in categorical_cols:\n",
        "\n",
        "    value_counts = df[col].str.lower().value_counts() if df[col].dtype == 'object' else df[col].value_counts()\n",
        "    original_counts = df[col].value_counts()\n",
        "\n",
        "    if len(value_counts) < len(original_counts):\n",
        "        print(f\"Column '{col}' has inconsistent capitalization\")\n",
        "\n",
        "    if df[col].dtype == 'object':\n",
        "        stripped_values = df[col].str.strip()\n",
        "        if not (stripped_values == df[col]).all():\n",
        "            print(f\"Column '{col}' has values with leading/trailing spaces\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MUoLPVehfOhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize=(16,16))\n",
        "\n",
        "\n",
        "numeric_df = df.select_dtypes(include=['int64', 'float64', 'boolean'])\n",
        "\n",
        "\n",
        "corr_matrix = numeric_df.corr()\n",
        "\n",
        "\n",
        "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
        "\n",
        "\n",
        "plt.figure(figsize=(16,16))\n",
        "\n",
        "\n",
        "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
        "\n",
        "\n",
        "sns.heatmap(corr_matrix, mask=mask, cmap=cmap, vmax=1, vmin=-1, center=0,\n",
        "            square=True, linewidths=.5, annot=True, fmt=\".2f\", annot_kws={\"size\": 8})\n",
        "\n",
        "plt.title('Correlation Matrix of Features', fontsize=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(16,16))\n",
        "\n",
        "\n",
        "threshold = 0.3\n",
        "\n",
        "\n",
        "corr_matrix_thresholded = corr_matrix.copy()\n",
        "corr_matrix_thresholded[abs(corr_matrix) < threshold] = 0\n",
        "\n",
        "\n",
        "mask = np.triu(np.ones_like(corr_matrix_thresholded, dtype=bool))\n",
        "\n",
        "\n",
        "sns.heatmap(corr_matrix_thresholded, mask=mask, cmap=cmap, vmax=1, vmin=-1, center=0,\n",
        "            square=True, linewidths=.5, annot=True, fmt=\".2f\", annot_kws={\"size\": 8})\n",
        "\n",
        "plt.title(f'Correlation Matrix (Only Showing Correlations > {threshold})', fontsize=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "if 'stage' in df.columns:\n",
        "    plt.figure(figsize=(16,16))\n",
        "\n",
        "\n",
        "    target_corr = corr_matrix['stage'].sort_values(ascending=False)\n",
        "\n",
        "\n",
        "    target_corr = target_corr.drop('stage')\n",
        "\n",
        "\n",
        "    sns.barplot(x=target_corr.values, y=target_corr.index)\n",
        "    plt.title('Feature Correlation with CKD Stage', fontsize=16)\n",
        "    plt.xlabel('Correlation Coefficient', fontsize=12)\n",
        "    plt.axvline(x=0, color='black', linestyle='--')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "1Um6GtKWfPnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "\n",
        "if 'stage' in numerical_cols:\n",
        "    numerical_cols.remove('stage')\n",
        "\n",
        "important_cols = [\n",
        "    'age', 'BMI', 'diastole_pressure', 'creatinine', 'hemoglobin',\n",
        "    'egfr', 'sodium', 'potassium', 'urea', 'albumin', 'blood_glucose',\n",
        "    'HBA1C', 'rbcc', 'wbcc', 'packed_cell_volume'\n",
        "]\n",
        "\n",
        "\n",
        "plot_cols = [col for col in important_cols if col in numerical_cols]\n",
        "\n",
        "if not plot_cols:\n",
        "    plot_cols = numerical_cols\n",
        "\n",
        "for i in range(0, len(plot_cols), 3):\n",
        "    subset_cols = plot_cols[i:i+3]\n",
        "\n",
        "\n",
        "    fig, axes = plt.subplots(1, len(subset_cols), figsize=(18, 6))\n",
        "\n",
        "\n",
        "    if len(subset_cols) == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "\n",
        "    for j, col in enumerate(subset_cols):\n",
        "        sns.boxplot(y=df[col], ax=axes[j])\n",
        "        axes[j].set_title(f'Boxplot of {col}')\n",
        "        axes[j].set_ylabel(col)\n",
        "\n",
        "\n",
        "        Q1 = df[col].quantile(0.25)\n",
        "        Q3 = df[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        outliers = df[(df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))][col]\n",
        "        outlier_count = len(outliers)\n",
        "        outlier_percent = (outlier_count / len(df)) * 100\n",
        "\n",
        "        axes[j].text(0.05, 0.95, f'Outliers: {outlier_count} ({outlier_percent:.1f}%)',\n",
        "                    transform=axes[j].transAxes, verticalalignment='top')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "sWjRxXelfWVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "if 'stage' in df.columns:\n",
        "\n",
        "    stage_counts = df['stage'].value_counts().sort_index()\n",
        "\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "\n",
        "    sns.countplot(x='stage', data=df, ax=ax1, palette='viridis')\n",
        "    ax1.set_title('Distribution of CKD Stages', fontsize=16)\n",
        "    ax1.set_xlabel('CKD Stage', fontsize=12)\n",
        "    ax1.set_ylabel('Count', fontsize=12)\n",
        "\n",
        "\n",
        "    for p in ax1.patches:\n",
        "        ax1.annotate(f'{int(p.get_height())}',\n",
        "                    (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                    ha = 'center', va = 'bottom', fontsize=10)\n",
        "\n",
        "\n",
        "    ax2.pie(stage_counts, labels=stage_counts.index, autopct='%1.1f%%',\n",
        "            startangle=90, shadow=True, explode=[0.05]*len(stage_counts),\n",
        "            colors=sns.color_palette('viridis', len(stage_counts)))\n",
        "    ax2.set_title('Percentage Distribution of CKD Stages', fontsize=16)\n",
        "    ax2.axis('equal')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    if 'gender' in df.columns:\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "\n",
        "        if df['gender'].dtype == 'boolean':\n",
        "            gender_col = df['gender'].map({True: 'Male', False: 'Female'})\n",
        "        else:\n",
        "            gender_col = df['gender']\n",
        "\n",
        "\n",
        "        stage_gender = pd.crosstab(df['stage'], gender_col)\n",
        "\n",
        "\n",
        "        stage_gender.plot(kind='bar', stacked=True, ax=plt.gca())\n",
        "        plt.title('CKD Stage Distribution by Gender', fontsize=16)\n",
        "        plt.xlabel('CKD Stage', fontsize=12)\n",
        "        plt.ylabel('Count', fontsize=12)\n",
        "        plt.legend(title='Gender')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    if 'age' in df.columns:\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "\n",
        "        age_bins = [0, 18, 35, 50, 65, 80, 100]\n",
        "        age_labels = ['<18', '18-35', '36-50', '51-65', '66-80', '>80']\n",
        "        df['age_group'] = pd.cut(df['age'], bins=age_bins, labels=age_labels)\n",
        "\n",
        "\n",
        "        stage_age = pd.crosstab(df['stage'], df['age_group'])\n",
        "\n",
        "\n",
        "        stage_age.plot(kind='bar', stacked=True, ax=plt.gca())\n",
        "        plt.title('CKD Stage Distribution by Age Group', fontsize=16)\n",
        "        plt.xlabel('CKD Stage', fontsize=12)\n",
        "        plt.ylabel('Count', fontsize=12)\n",
        "        plt.legend(title='Age Group')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\"'stage' column not found in the dataframe. Please check your column names.\")"
      ],
      "metadata": {
        "id": "6tMa8Smufb-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATA PREPROCESSING"
      ],
      "metadata": {
        "id": "6rjKD1H0f-fz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Handle inconsistent data\n",
        "def handle_inconsistent_data(df):\n",
        "    print(\"\\n1. Handling inconsistent data...\")\n",
        "\n",
        "\n",
        "    df_clean = df.copy()\n",
        "\n",
        "\n",
        "    numeric_cols = ['BMI', 'diastolic_bp', 'creatinine', 'hemoglobin', 'egfr',\n",
        "                   'sodium', 'potassium', 'urea', 'albumin', 'blood_glucose',\n",
        "                   'HbA1C', 'rbcc', 'wbcc', 'packed_cells']\n",
        "\n",
        "\n",
        "    numeric_cols = [col for col in numeric_cols if col in df.columns]\n",
        "\n",
        "    for col in numeric_cols:\n",
        "\n",
        "        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
        "\n",
        "\n",
        "    if 'diabetes' in df_clean.columns and 'HbA1C' in df_clean.columns:\n",
        "        inconsistent = df_clean[(df_clean['diabetes'] == 1) & (df_clean['HbA1C'] < 6.5)]\n",
        "        print(f\"Found {len(inconsistent)} patients with diabetes but HbA1C < 6.5\")\n",
        "\n",
        "    if 'hypertension' in df_clean.columns and 'diastolic_bp' in df_clean.columns:\n",
        "        inconsistent = df_clean[(df_clean['hypertension'] == 1) & (df_clean['diastolic_bp'] < 80)]\n",
        "        print(f\"Found {len(inconsistent)} patients with hypertension but diastolic_bp < 80\")\n",
        "\n",
        "\n",
        "    if 'stage' in df_clean.columns and 'egfr' in df_clean.columns:\n",
        "\n",
        "        stage_egfr_inconsistencies = 0\n",
        "\n",
        "        for _, row in df_clean.iterrows():\n",
        "            stage = row['stage']\n",
        "            egfr = row['egfr']\n",
        "\n",
        "            if (stage == 1 and egfr < 90) or \\\n",
        "               (stage == 2 and (egfr < 60 or egfr >= 90)) or \\\n",
        "               (stage == 3 and (egfr < 30 or egfr >= 60)) or \\\n",
        "               (stage == 4 and (egfr < 15 or egfr >= 30)) or \\\n",
        "               (stage == 5 and egfr >= 15):\n",
        "                stage_egfr_inconsistencies += 1\n",
        "\n",
        "        print(f\"Found {stage_egfr_inconsistencies} inconsistencies between stage and eGFR values\")\n",
        "\n",
        "    return df_clean\n",
        "\n",
        "# 2. Feature engineering\n",
        "def perform_feature_engineering(df):\n",
        "    print(\"\\n2. Performing feature engineering...\")\n",
        "\n",
        "    df_enhanced = df.copy()\n",
        "\n",
        "    # albumin to creatinine ratio\n",
        "    if 'albumin' in df.columns and 'creatinine' in df.columns:\n",
        "        df_enhanced['albumin_creatinine_ratio'] = df['albumin'] / df['creatinine']\n",
        "        print(\"Created new feature: albumin_creatinine_ratio\")\n",
        "\n",
        "\n",
        "        df_enhanced['albumin_creatinine_ratio'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "        if df_enhanced['albumin_creatinine_ratio'].isnull().sum() > 0:\n",
        "            df_enhanced['albumin_creatinine_ratio'].fillna(df_enhanced['albumin_creatinine_ratio'].median(), inplace=True)\n",
        "            print(f\"Replaced {df_enhanced['albumin_creatinine_ratio'].isnull().sum()} NaN values in albumin_creatinine_ratio\")\n",
        "\n",
        "        print(\"\\nAlbumin to Creatinine Ratio statistics:\")\n",
        "        print(df_enhanced['albumin_creatinine_ratio'].describe())\n",
        "    else:\n",
        "        print(\"Warning: Cannot create albumin_creatinine_ratio, missing required columns\")\n",
        "\n",
        "    return df_enhanced\n",
        "\n",
        "\n",
        "# 3. Z-score standardization\n",
        "def perform_z_score_standardization(df):\n",
        "    print(\"\\n3. Performing Z-score standardization...\")\n",
        "\n",
        "    df_standardized = df.copy()\n",
        "\n",
        "\n",
        "    numeric_cols = ['BMI', 'diastolic_bp', 'creatinine', 'hemoglobin', 'egfr',\n",
        "                   'sodium', 'potassium', 'urea', 'albumin', 'blood_glucose',\n",
        "                   'HbA1C', 'rbcc', 'wbcc', 'packed_cells']\n",
        "\n",
        "\n",
        "    if 'albumin_creatinine_ratio' in df.columns:\n",
        "        numeric_cols.append('albumin_creatinine_ratio')\n",
        "\n",
        "\n",
        "    numeric_cols = [col for col in numeric_cols if col in df.columns]\n",
        "\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "\n",
        "    scaled_data = scaler.fit_transform(df_standardized[numeric_cols])\n",
        "\n",
        "\n",
        "    for i, col in enumerate(numeric_cols):\n",
        "        df_standardized[f'{col}_standardized'] = scaled_data[:, i]\n",
        "\n",
        "    print(f\"Added Z-score standardized columns for {len(numeric_cols)} numerical features\")\n",
        "\n",
        "\n",
        "    with open('standard_scaler.pkl', 'wb') as file:\n",
        "      pickle.dump(scaler, file)\n",
        "    print(\"Scaler saved successfully!\")\n",
        "\n",
        "    return df_standardized\n",
        "\n",
        "\n",
        "\n",
        "# 4. Handle outliers\n",
        "def handle_outliers(df):\n",
        "    print(\"\\n4. Handling outliers...\")\n",
        "\n",
        "    df_no_outliers = df.copy()\n",
        "\n",
        "\n",
        "    numeric_cols = ['BMI', 'diastolic_bp', 'creatinine', 'hemoglobin', 'egfr',\n",
        "                   'sodium', 'potassium', 'urea', 'albumin', 'blood_glucose',\n",
        "                   'HbA1C', 'rbcc', 'wbcc', 'packed_cells']\n",
        "\n",
        "\n",
        "    if 'albumin_creatinine_ratio' in df.columns:\n",
        "        numeric_cols.append('albumin_creatinine_ratio')\n",
        "\n",
        "\n",
        "    numeric_cols = [col for col in numeric_cols if col in df.columns]\n",
        "\n",
        "\n",
        "    outlier_counts = {}\n",
        "\n",
        "    for col in numeric_cols:\n",
        "\n",
        "        Q1 = df[col].quantile(0.25)\n",
        "        Q3 = df[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "\n",
        "\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "\n",
        "        outliers_lower = (df[col] < lower_bound).sum()\n",
        "        outliers_upper = (df[col] > upper_bound).sum()\n",
        "        outlier_counts[col] = {'lower': outliers_lower, 'upper': outliers_upper, 'total': outliers_lower + outliers_upper}\n",
        "\n",
        "\n",
        "        df_no_outliers[col] = df[col].clip(lower=lower_bound, upper=upper_bound)\n",
        "\n",
        "        print(f\"{col}: Q1={Q1:.2f}, Q3={Q3:.2f}, IQR={IQR:.2f}\")\n",
        "        print(f\"  Lower bound: {lower_bound:.2f}, Upper bound: {upper_bound:.2f}\")\n",
        "        print(f\"  Outliers: {outlier_counts[col]['total']} ({outlier_counts[col]['lower']} below, {outlier_counts[col]['upper']} above)\")\n",
        "\n",
        "\n",
        "    total_outliers = sum(info['total'] for info in outlier_counts.values())\n",
        "    print(f\"\\nTotal outliers found and capped: {total_outliers}\")\n",
        "\n",
        "    return df_no_outliers\n",
        "\n",
        "\n",
        "\n",
        "# 5. Handle class imbalance with SMOTE for multiclass\n",
        "def handle_class_imbalance(df, target_column='stage'):\n",
        "    print(f\"\\n5. Handling class imbalance for {target_column} using SMOTE...\")\n",
        "\n",
        "    if target_column not in df.columns:\n",
        "        print(f\"Warning: Target column '{target_column}' not found in dataset\")\n",
        "        return df\n",
        "\n",
        "\n",
        "    class_counts = df[target_column].value_counts().sort_index()\n",
        "    print(\"Original class distribution:\")\n",
        "    print(class_counts)\n",
        "\n",
        "\n",
        "    if len(class_counts) <= 1:\n",
        "        print(\"Only one class found, SMOTE not applicable\")\n",
        "        return df\n",
        "\n",
        "\n",
        "    X = df.drop(columns=[target_column])\n",
        "    y = df[target_column]\n",
        "\n",
        "\n",
        "    X = X.select_dtypes(include=['number'])\n",
        "\n",
        "\n",
        "    if X.isnull().any().any():\n",
        "        print(f\"Found NaN values in the data. Imputing missing values before applying SMOTE...\")\n",
        "        # Use SimpleImputer to replace NaN values with median\n",
        "        from sklearn.impute import SimpleImputer\n",
        "        imputer = SimpleImputer(strategy='median')\n",
        "        X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
        "        print(f\"Imputation complete. NaN values replaced with median values.\")\n",
        "\n",
        "\n",
        "    try:\n",
        "\n",
        "        smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
        "        X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "\n",
        "        df_balanced = pd.DataFrame(X_resampled, columns=X.columns)\n",
        "        df_balanced[target_column] = y_resampled\n",
        "\n",
        "\n",
        "        new_class_counts = df_balanced[target_column].value_counts().sort_index()\n",
        "        print(\"Balanced class distribution:\")\n",
        "        print(new_class_counts)\n",
        "\n",
        "        print(f\"Original dataset: {len(df)} samples\")\n",
        "        print(f\"Balanced dataset: {len(df_balanced)} samples\")\n",
        "\n",
        "        return df_balanced\n",
        "    except Exception as e:\n",
        "        print(f\"Error applying SMOTE: {e}\")\n",
        "        print(\"Returning original dataset\")\n",
        "        return df\n",
        "\n",
        "\n",
        "\n",
        "def preprocess_data(df):\n",
        "    print(\"Starting data preprocessing...\")\n",
        "\n",
        "    # 1. Handle inconsistent data\n",
        "    df_clean = handle_inconsistent_data(df)\n",
        "\n",
        "    # 2. Feature engineering\n",
        "    df_enhanced = perform_feature_engineering(df_clean)\n",
        "\n",
        "    # 3. Z-score standardization\n",
        "    df_standardized = perform_z_score_standardization(df_enhanced)\n",
        "\n",
        "    # 4. Handle outliers\n",
        "    df_no_outliers = handle_outliers(df_standardized)\n",
        "\n",
        "    # 5. Handle class imbalance with SMOTE\n",
        "    df_balanced = handle_class_imbalance(df_no_outliers, target_column='stage')\n",
        "\n",
        "    print(\"\\nPreprocessing complete!\")\n",
        "    print(f\"Original dataset shape: {df.shape}\")\n",
        "    print(f\"Processed dataset shape: {df_balanced.shape}\")\n",
        "\n",
        "    # Save processed data\n",
        "    df_balanced.to_csv('processed_healthcare_data.csv', index=False)\n",
        "    print(\"Processed data saved to 'processed_healthcare_data.csv'\")\n",
        "\n",
        "    return df_balanced\n",
        "\n",
        "\n",
        "processed_df = preprocess_data(df)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "\n",
        "if 'albumin_creatinine_ratio' in processed_df.columns:\n",
        "    plt.subplot(2, 2, 1)\n",
        "    sns.histplot(processed_df['albumin_creatinine_ratio'], kde=True)\n",
        "    plt.title('Distribution of Albumin to Creatinine Ratio')\n",
        "    plt.xlabel('Ratio Value')\n",
        "    plt.ylabel('Frequency')\n",
        "\n",
        "\n",
        "if 'albumin_creatinine_ratio' in processed_df.columns and 'stage' in processed_df.columns:\n",
        "    plt.subplot(2, 2, 2)\n",
        "    sns.boxplot(data=processed_df, x='stage', y='albumin_creatinine_ratio')\n",
        "    plt.title('Albumin to Creatinine Ratio by Kidney Disease Stage')\n",
        "    plt.xlabel('Stage')\n",
        "    plt.ylabel('Albumin to Creatinine Ratio')\n",
        "\n",
        "\n",
        "if 'albumin' in processed_df.columns and 'creatinine' in processed_df.columns and 'stage' in processed_df.columns:\n",
        "    plt.subplot(2, 2, 3)\n",
        "    sns.scatterplot(data=processed_df, x='albumin', y='creatinine', hue='stage', palette='viridis')\n",
        "    plt.title('Albumin vs Creatinine by Kidney Disease Stage')\n",
        "    plt.xlabel('Albumin')\n",
        "    plt.ylabel('Creatinine')\n",
        "\n",
        "\n",
        "if 'creatinine' in processed_df.columns and 'stage' in processed_df.columns:\n",
        "    plt.subplot(2, 2, 4)\n",
        "    sns.boxplot(data=processed_df, x='stage', y='creatinine')\n",
        "    plt.title('Creatinine by Kidney Disease Stage')\n",
        "    plt.xlabel('Stage')\n",
        "    plt.ylabel('Creatinine')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "\n",
        "if 'albumin_standardized' in processed_df.columns:\n",
        "    plt.subplot(2, 2, 1)\n",
        "    sns.histplot(processed_df['albumin_standardized'], kde=True)\n",
        "    plt.title('Distribution of Standardized Albumin')\n",
        "    plt.xlabel('Z-Score')\n",
        "    plt.ylabel('Frequency')\n",
        "\n",
        "\n",
        "if 'creatinine_standardized' in processed_df.columns:\n",
        "    plt.subplot(2, 2, 2)\n",
        "    sns.histplot(processed_df['creatinine_standardized'], kde=True)\n",
        "    plt.title('Distribution of Standardized Creatinine')\n",
        "    plt.xlabel('Z-Score')\n",
        "    plt.ylabel('Frequency')\n",
        "\n",
        "\n",
        "if 'albumin_creatinine_ratio_standardized' in processed_df.columns:\n",
        "    plt.subplot(2, 2, 3)\n",
        "    sns.histplot(processed_df['albumin_creatinine_ratio_standardized'], kde=True)\n",
        "    plt.title('Distribution of Standardized Albumin to Creatinine Ratio')\n",
        "    plt.xlabel('Z-Score')\n",
        "    plt.ylabel('Frequency')\n",
        "\n",
        "\n",
        "if 'stage' in processed_df.columns:\n",
        "    plt.subplot(2, 2, 4)\n",
        "    sns.countplot(x='stage', data=processed_df)\n",
        "    plt.title('Distribution of Kidney Disease Stages After SMOTE')\n",
        "    plt.xlabel('Stage')\n",
        "    plt.ylabel('Count')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "bKh0gcO1fgEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Train Validation Test Split**"
      ],
      "metadata": {
        "id": "pwQyOvgVfvEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_processed=pd.read_csv('/content/processed_healthcare_data (1).csv')\n",
        "df_processed.head()"
      ],
      "metadata": {
        "id": "QqXR0qrLfonQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(df, target_column='stage', train_size=0.7, val_size=0.15, test_size=0.15, random_state=42):\n",
        "\n",
        "\n",
        "    assert abs(train_size + val_size + test_size - 1.0) < 1e-10, \"Split proportions must sum to 1\"\n",
        "\n",
        "\n",
        "    df = df.dropna(subset=[target_column])  # Drop rows with NaN in 'stage'\n",
        "\n",
        "\n",
        "    X = df.drop(columns=[target_column])\n",
        "    y = df[target_column]\n",
        "\n",
        "\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "        X, y, test_size=(val_size + test_size), random_state=random_state, stratify=y\n",
        "    )\n",
        "\n",
        "\n",
        "    test_size_adjusted = test_size / (val_size + test_size)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(\n",
        "        X_temp, y_temp, test_size=test_size_adjusted, random_state=random_state, stratify=y_temp\n",
        "    )\n",
        "\n",
        "    print(f\"Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(df):.1%})\")\n",
        "    print(f\"Validation set: {X_val.shape[0]} samples ({X_val.shape[0]/len(df):.1%})\")\n",
        "    print(f\"Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(df):.1%})\")\n",
        "\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = split_data(df_processed, target_column='stage')\n",
        "\n",
        "\n",
        "def check_class_distribution(y_train, y_val, y_test):\n",
        "    \"\"\"Check the distribution of classes in each split\"\"\"\n",
        "\n",
        "\n",
        "    train_counts = pd.Series(y_train).value_counts().sort_index()\n",
        "    val_counts = pd.Series(y_val).value_counts().sort_index()\n",
        "    test_counts = pd.Series(y_test).value_counts().sort_index()\n",
        "\n",
        "\n",
        "    train_pct = train_counts / train_counts.sum() * 100\n",
        "    val_pct = val_counts / val_counts.sum() * 100\n",
        "    test_pct = test_counts / test_counts.sum() * 100\n",
        "\n",
        "\n",
        "    distribution_df = pd.DataFrame({\n",
        "        'Train Count': train_counts,\n",
        "        'Train %': train_pct,\n",
        "        'Val Count': val_counts,\n",
        "        'Val %': val_pct,\n",
        "        'Test Count': test_counts,\n",
        "        'Test %': test_pct\n",
        "    })\n"
      ],
      "metadata": {
        "id": "udYOq0OOgC5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DNN (Deep Neural Network)"
      ],
      "metadata": {
        "id": "TeN5wc4HgYQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "X_train_np = X_train.values\n",
        "X_val_np = X_val.values\n",
        "X_test_np = X_test.values\n",
        "y_train_np = y_train.values\n",
        "y_val_np = y_val.values\n",
        "y_test_np = y_test.values\n",
        "\n",
        "\n",
        "all_targets = np.concatenate([y_train_np, y_val_np, y_test_np])\n",
        "unique_classes = np.unique(all_targets)\n",
        "print(f\"Unique class values in the dataset: {unique_classes}\")\n",
        "num_classes = len(unique_classes)\n",
        "print(f\"Total number of classes: {num_classes}\")\n",
        "\n",
        "\n",
        "class_mapping = {original: idx for idx, original in enumerate(sorted(unique_classes))}\n",
        "print(f\"Class mapping: {class_mapping}\")\n",
        "\n",
        "y_train_mapped = np.array([class_mapping[y] for y in y_train_np])\n",
        "y_val_mapped = np.array([class_mapping[y] for y in y_val_np])\n",
        "y_test_mapped = np.array([class_mapping[y] for y in y_test_np])\n",
        "\n",
        "\n",
        "X_train_tensor = torch.FloatTensor(X_train_np)\n",
        "y_train_tensor = torch.LongTensor(y_train_mapped)\n",
        "X_val_tensor = torch.FloatTensor(X_val_np)\n",
        "y_val_tensor = torch.LongTensor(y_val_mapped)\n",
        "X_test_tensor = torch.FloatTensor(X_test_np)\n",
        "y_test_tensor = torch.LongTensor(y_test_mapped)\n",
        "\n",
        "\n",
        "class CKDDataset(Dataset):\n",
        "    def __init__(self, features, targets):\n",
        "        self.features = features\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.targets[idx]\n",
        "\n",
        "train_dataset = CKDDataset(X_train_tensor, y_train_tensor)\n",
        "val_dataset = CKDDataset(X_val_tensor, y_val_tensor)\n",
        "test_dataset = CKDDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "\n",
        "class CKDModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes, num_classes):\n",
        "        super(CKDModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_sizes[0])\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(hidden_sizes[1], hidden_sizes[2])\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc4 = nn.Linear(hidden_sizes[2], hidden_sizes[3])\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.fc5 = nn.Linear(hidden_sizes[3], num_classes)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu1(self.fc1(x))\n",
        "        x = self.relu2(self.fc2(x))\n",
        "        x = self.relu3(self.fc3(x))\n",
        "        x = self.relu4(self.fc4(x))\n",
        "        x = self.fc5(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "input_size = X_train_np.shape[1]\n",
        "\n",
        "hidden_sizes = [128, 64, 32, 16]\n",
        "\n",
        "\n",
        "model = CKDModel(input_size, hidden_sizes, num_classes)\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=50):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_acc = correct / total\n",
        "        train_losses.append(epoch_loss)\n",
        "        train_accs.append(epoch_acc)\n",
        "\n",
        "\n",
        "        model.eval()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_loss = running_loss / len(val_loader.dataset)\n",
        "        epoch_acc = correct / total\n",
        "        val_losses.append(epoch_loss)\n",
        "        val_accs.append(epoch_acc)\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accs[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_accs[-1]:.4f}')\n",
        "\n",
        "    return train_losses, val_losses, train_accs, val_accs\n",
        "\n",
        "\n",
        "print(\"Training the model...\")\n",
        "train_losses, val_losses, train_accs, val_accs = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=50)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accs, label='Training Accuracy')\n",
        "plt.plot(val_accs, label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    return np.array(all_preds), np.array(all_labels)\n",
        "\n",
        "\n",
        "print(\"Evaluating the model on test set...\")\n",
        "y_pred, y_true = evaluate_model(model, test_loader)\n",
        "\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "reverse_mapping = {idx: original for original, idx in class_mapping.items()}\n",
        "class_labels = [reverse_mapping[i] for i in range(num_classes)]\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_labels,\n",
        "            yticklabels=class_labels)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=[f'Stage {reverse_mapping[i]}' for i in range(num_classes)]))\n",
        "\n",
        "y_pred_original = np.array([reverse_mapping[y] for y in y_pred])\n",
        "y_true_original = np.array([reverse_mapping[y] for y in y_true])\n",
        "\n",
        "\n",
        "test_accuracy = np.mean(y_pred == y_true)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "JEE2d5CFgaCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def check_overfitting(train_losses, val_losses, train_accs, val_accs):\n",
        "    \"\"\"\n",
        "    Analyze training and validation metrics to detect overfitting\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    loss_gaps = [train - val for train, val in zip(train_losses, val_losses)]\n",
        "    acc_gaps = [train - val for train, val in zip(train_accs, val_accs)]\n",
        "\n",
        "\n",
        "    val_loss_increasing = False\n",
        "    for i in range(5, len(val_losses)):\n",
        "\n",
        "        if all(val_losses[i-j] < val_losses[i-j+1] for j in range(1, 5)) and train_losses[i] < train_losses[i-5]:\n",
        "            val_loss_increasing = True\n",
        "            break\n",
        "\n",
        "\n",
        "    significant_acc_gap = any(gap > 0.1 for gap in acc_gaps[-5:])\n",
        "\n",
        "\n",
        "    high_train_low_val_acc = train_accs[-1] > 0.95 and val_accs[-1] < 0.85\n",
        "\n",
        "\n",
        "    is_overfitting = val_loss_increasing or significant_acc_gap or high_train_low_val_acc\n",
        "\n",
        "\n",
        "    analysis = {\n",
        "        \"final_train_loss\": train_losses[-1],\n",
        "        \"final_val_loss\": val_losses[-1],\n",
        "        \"final_train_acc\": train_accs[-1],\n",
        "        \"final_val_acc\": val_accs[-1],\n",
        "        \"final_loss_gap\": loss_gaps[-1],\n",
        "        \"final_acc_gap\": acc_gaps[-1],\n",
        "        \"val_loss_increasing\": val_loss_increasing,\n",
        "        \"significant_acc_gap\": significant_acc_gap,\n",
        "        \"high_train_low_val_acc\": high_train_low_val_acc\n",
        "    }\n",
        "\n",
        "    return is_overfitting, analysis\n",
        "\n",
        "\n",
        "def visualize_overfitting(train_losses, val_losses, train_accs, val_accs):\n",
        "    \"\"\"\n",
        "    Create detailed visualizations to analyze overfitting\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "\n",
        "\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(train_accs, label='Training Accuracy')\n",
        "    plt.plot(val_accs, label='Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "\n",
        "    plt.subplot(2, 2, 3)\n",
        "    loss_gaps = [train - val for train, val in zip(train_losses, val_losses)]\n",
        "    plt.plot(loss_gaps)\n",
        "    plt.axhline(y=0, color='r', linestyle='--')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Train Loss - Val Loss')\n",
        "    plt.title('Loss Gap (negative values indicate underfitting)')\n",
        "\n",
        "\n",
        "    plt.subplot(2, 2, 4)\n",
        "    acc_gaps = [train - val for train, val in zip(train_accs, val_accs)]\n",
        "    plt.plot(acc_gaps)\n",
        "    plt.axhline(y=0, color='r', linestyle='--')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Train Acc - Val Acc')\n",
        "    plt.title('Accuracy Gap (large positive values indicate overfitting)')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "class RegularizedCKDModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes, num_classes, dropout_rate=0.3):\n",
        "        super(RegularizedCKDModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_sizes[0])\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_sizes[0])\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "        self.relu1 = nn.ReLU()\n",
        "\n",
        "        self.fc2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_sizes[1])\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "        self.relu2 = nn.ReLU()\n",
        "\n",
        "        self.fc3 = nn.Linear(hidden_sizes[1], hidden_sizes[2])\n",
        "        self.bn3 = nn.BatchNorm1d(hidden_sizes[2])\n",
        "        self.dropout3 = nn.Dropout(dropout_rate)\n",
        "        self.relu3 = nn.ReLU()\n",
        "\n",
        "        self.fc4 = nn.Linear(hidden_sizes[2], hidden_sizes[3])\n",
        "        self.bn4 = nn.BatchNorm1d(hidden_sizes[3])\n",
        "        self.dropout4 = nn.Dropout(dropout_rate)\n",
        "        self.relu4 = nn.ReLU()\n",
        "\n",
        "        self.fc5 = nn.Linear(hidden_sizes[3], num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu1(self.bn1(self.fc1(x)))\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        x = self.relu2(self.bn2(self.fc2(x)))\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        x = self.relu3(self.bn3(self.fc3(x)))\n",
        "        x = self.dropout3(x)\n",
        "\n",
        "        x = self.relu4(self.bn4(self.fc4(x)))\n",
        "        x = self.dropout4(x)\n",
        "\n",
        "        x = self.fc5(x)\n",
        "        return x\n",
        "\n",
        "def train_with_early_stopping(model, train_loader, val_loader, criterion, optimizer,\n",
        "                             num_epochs=100, patience=10, l2_lambda=0.001):\n",
        "    \"\"\"\n",
        "    Train model with early stopping and L2 regularization\n",
        "\n",
        "    Args:\n",
        "        patience: Number of epochs to wait for improvement before stopping\n",
        "        l2_lambda: L2 regularization strength\n",
        "    \"\"\"\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state = None\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "\n",
        "            l2_reg = 0.0\n",
        "            for param in model.parameters():\n",
        "                l2_reg += torch.norm(param, 2)\n",
        "            loss += l2_lambda * l2_reg\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_acc = correct / total\n",
        "        train_losses.append(epoch_loss)\n",
        "        train_accs.append(epoch_acc)\n",
        "\n",
        "\n",
        "        model.eval()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_loss = running_loss / len(val_loader.dataset)\n",
        "        epoch_acc = correct / total\n",
        "        val_losses.append(epoch_loss)\n",
        "        val_accs.append(epoch_acc)\n",
        "\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accs[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_accs[-1]:.4f}')\n",
        "\n",
        "\n",
        "        if val_losses[-1] < best_val_loss:\n",
        "            best_val_loss = val_losses[-1]\n",
        "            best_model_state = model.state_dict().copy()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f'Early stopping at epoch {epoch+1}')\n",
        "\n",
        "                model.load_state_dict(best_model_state)\n",
        "                break\n",
        "\n",
        "\n",
        "    if epoch == num_epochs - 1 and best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "\n",
        "    return train_losses, val_losses, train_accs, val_accs, model\n",
        "\n",
        "\n",
        "def train_regularized_model(X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor,\n",
        "                           input_size, hidden_sizes, num_classes):\n",
        "    \"\"\"\n",
        "    Train a model with regularization techniques to prevent overfitting\n",
        "    \"\"\"\n",
        "\n",
        "    train_dataset = CKDDataset(X_train_tensor, y_train_tensor)\n",
        "    val_dataset = CKDDataset(X_val_tensor, y_val_tensor)\n",
        "\n",
        "    batch_size = 32\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "\n",
        "    model = RegularizedCKDModel(input_size, hidden_sizes, num_classes, dropout_rate=0.3)\n",
        "\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)  # weight_decay adds L2 regularization\n",
        "\n",
        "\n",
        "    train_losses, val_losses, train_accs, val_accs, model = train_with_early_stopping(\n",
        "        model, train_loader, val_loader, criterion, optimizer, num_epochs=100, patience=10\n",
        "    )\n",
        "\n",
        "\n",
        "    is_overfitting, analysis = check_overfitting(train_losses, val_losses, train_accs, val_accs)\n",
        "\n",
        "    return model, train_losses, val_losses, train_accs, val_accs, is_overfitting, analysis\n",
        "\n",
        "\n",
        "is_overfitting, analysis = check_overfitting(train_losses, val_losses, train_accs, val_accs)\n",
        "print(f\"Is the model overfitting? {is_overfitting}\")\n",
        "print(\"Analysis:\", analysis)\n",
        "\n",
        "\n",
        "visualize_overfitting(train_losses, val_losses, train_accs, val_accs)\n",
        "\n",
        "\n",
        "if is_overfitting:\n",
        "    print(\"Overfitting detected! Training a regularized model...\")\n",
        "\n",
        "    regularized_model, reg_train_losses, reg_val_losses, reg_train_accs, reg_val_accs, still_overfitting, reg_analysis = train_regularized_model(\n",
        "        X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor,\n",
        "        input_size, hidden_sizes, num_classes\n",
        "     )\n",
        "\n",
        "    print(f\"Is the regularized model still overfitting? {still_overfitting}\")\n",
        "    visualize_overfitting(reg_train_losses, reg_val_losses, reg_train_accs, reg_val_accs)"
      ],
      "metadata": {
        "id": "WsP7mRrQgp-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "def plot_roc_auc_curve(model, test_loader, num_classes):\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_probs = np.array(all_probs)\n",
        "\n",
        "\n",
        "    fpr = {}\n",
        "    tpr = {}\n",
        "    roc_auc = {}\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    for i in range(num_classes):\n",
        "\n",
        "        y_true_binary = (all_labels == i).astype(int)\n",
        "        y_score = all_probs[:, i]\n",
        "\n",
        "        fpr[i], tpr[i], _ = roc_curve(y_true_binary, y_score)\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "        plt.plot(fpr[i], tpr[i], lw=2,\n",
        "                 label=f'ROC curve of class {i} (area = {roc_auc[i]:.2f})')\n",
        "\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Multi-class ROC Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('roc_auc_curve.png')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    macro_roc_auc = np.mean(list(roc_auc.values()))\n",
        "    print(f\"Macro-average ROC AUC: {macro_roc_auc:.4f}\")\n",
        "\n",
        "    return fpr, tpr, roc_auc\n",
        "\n",
        "print(\"\\nGenerating ROC AUC curve...\")\n",
        "fpr, tpr, roc_auc = plot_roc_auc_curve(model, test_loader, num_classes)"
      ],
      "metadata": {
        "id": "8Xeph5Yzgykp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_learning_curve_simplified(model, train_dataset, val_dataset, class_mapping,\n",
        "                                  train_sizes=np.linspace(0.2, 1.0, 5),\n",
        "                                  batch_size=32,\n",
        "                                  num_epochs=30):\n",
        "    \"\"\"\n",
        "    Generate learning curve by training on increasing subsets of the training data\n",
        "    \"\"\"\n",
        "\n",
        "    original_state = model.state_dict().copy()\n",
        "\n",
        "    train_scores = []\n",
        "    val_scores = []\n",
        "\n",
        "    for train_size in train_sizes:\n",
        "\n",
        "        n_samples = int(len(train_dataset) * train_size)\n",
        "        print(f\"Training with {n_samples} samples ({train_size:.1%} of training data)\")\n",
        "\n",
        "\n",
        "        indices = torch.randperm(len(train_dataset))[:n_samples]\n",
        "        subset_train = torch.utils.data.Subset(train_dataset, indices)\n",
        "\n",
        "\n",
        "        train_loader = torch.utils.data.DataLoader(subset_train, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "\n",
        "        model.load_state_dict(original_state)\n",
        "\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "        best_val_acc = 0\n",
        "        for epoch in range(num_epochs):\n",
        "\n",
        "            model.train()\n",
        "            running_loss = 0.0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "\n",
        "            for inputs, labels in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "            train_acc = correct / total\n",
        "\n",
        "\n",
        "            model.eval()\n",
        "            correct = 0\n",
        "            total = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in val_loader:\n",
        "                    outputs = model(inputs)\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()\n",
        "\n",
        "            val_acc = correct / total\n",
        "\n",
        "\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "\n",
        "\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                print(f'  Epoch {epoch+1}/{num_epochs}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}')\n",
        "\n",
        "\n",
        "        train_scores.append(train_acc)\n",
        "        val_scores.append(best_val_acc)\n",
        "\n",
        "        print(f\"  Final: Train Acc: {train_acc:.4f}, Best Val Acc: {best_val_acc:.4f}\")\n",
        "\n",
        "\n",
        "    model.load_state_dict(original_state)\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train_sizes, train_scores, 'o-', label='Training accuracy')\n",
        "    plt.plot(train_sizes, val_scores, 's-', label='Validation accuracy')\n",
        "    plt.xlabel('Training Set Size (proportion)')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Learning Curve')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('learning_curve.png')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    results = []\n",
        "    for i, size in enumerate(train_sizes):\n",
        "        results.append({\n",
        "            'train_size': size,\n",
        "            'train_samples': int(len(train_dataset) * size),\n",
        "            'train_accuracy': train_scores[i],\n",
        "            'val_accuracy': val_scores[i]\n",
        "        })\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "    print(\"Learning Curve Results:\")\n",
        "    print(results_df)\n",
        "\n",
        "    return results_df\n",
        "\n",
        "\n",
        "print(\"\\nGenerating learning curve (simplified version)...\")\n",
        "learning_curve_results = plot_learning_curve_simplified(\n",
        "    model,\n",
        "    train_dataset,\n",
        "    val_dataset,\n",
        "    class_mapping,\n",
        "    train_sizes=np.linspace(0.2, 1.0, 5),\n",
        "    batch_size=32,\n",
        "    num_epochs=20\n",
        ")"
      ],
      "metadata": {
        "id": "K0PgWkBig8xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "unstandardized_continuous = ['age', 'diastole_pressure', 'packed_cell_volume', 'HBA1C']\n",
        "\n",
        "\n",
        "df_temp = df_processed.copy()\n",
        "scaler = StandardScaler()\n",
        "df_temp[['age_standardized', 'diastole_pressure_standardized',\n",
        "         'packed_cell_volume_standardized', 'hba1c_standardized']] = scaler.fit_transform(df_processed[unstandardized_continuous])\n",
        "\n",
        "\n",
        "original_continuous_cols = ['BMI', 'creatinine', 'creatinine', 'egfr', 'sodium', 'potassium',\n",
        "                           'urea', 'albumin', 'blood_glucose', 'rbcc', 'wbcc',\n",
        "                           'albumin_creatinine_ratio']\n",
        "\n",
        "binary_categorical_cols = ['gender', 'ethnicity', 'hypertension', 'anemia', 'appetite',\n",
        "                          'diabetes', 'coronary_heart_disease', 'acute_kidney_injury',\n",
        "                          'edema', 'Smoking', 'AlcoholDrinking', 'DiffWalking', 'Asthma']\n",
        "\n",
        "scaled_continuous_cols = ['BMI_standardized', 'creatinine_standardized',\n",
        "                         'hemoglobin_standardized', 'egfr_standardized',\n",
        "                         'sodium_standardized', 'potassium_standardized',\n",
        "                         'urea_standardized', 'albumin_standardized',\n",
        "                         'blood_glucose_standardized', 'rbcc_standardized',\n",
        "                         'wbcc_standardized', 'albumin_creatinine_ratio_standardized']\n",
        "\n",
        "new_scaled_cols = ['age_standardized', 'diastole_pressure_standardized',\n",
        "                  'packed_cell_volume_standardized', 'hba1c_standardized']\n",
        "\n",
        "updated_scaled_continuous_cols = scaled_continuous_cols + new_scaled_cols\n",
        "\n",
        "columns_to_keep = binary_categorical_cols + updated_scaled_continuous_cols\n",
        "\n",
        "X = df_temp[columns_to_keep]\n",
        "y = df_processed['stage']\n",
        "\n",
        "print(f\"Total features selected for autoencoder: {len(columns_to_keep)}\")\n",
        "print(f\"Binary/categorical features: {len(binary_categorical_cols)}\")\n",
        "print(f\"Scaled continuous features: {len(updated_scaled_continuous_cols)}\")"
      ],
      "metadata": {
        "id": "PS7fXLIeg9ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "feature_names = X.columns.tolist()\n",
        "\n",
        "\n",
        "X = X.values\n",
        "y = y.values\n",
        "\n",
        "\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1765, random_state=42, stratify=y_train_val)\n",
        "\n",
        "\n",
        "print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")\n",
        "\n",
        "\n",
        "X_train_scaled = X_train\n",
        "X_val_scaled = X_val\n",
        "X_test_scaled = X_test\n",
        "\n",
        "\n",
        "n_features = X_train.shape[1]\n",
        "print(f\"Number of features: {n_features}\")\n",
        "\n",
        "\n",
        "encoding_dim = min(32, n_features // 2)\n",
        "print(f\"Encoding dimension: {encoding_dim}\")\n",
        "\n",
        "\n",
        "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
        "X_val_tensor = torch.FloatTensor(X_val_scaled)\n",
        "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
        "y_train_tensor = torch.LongTensor(y_train)\n",
        "y_val_tensor = torch.LongTensor(y_val)\n",
        "y_test_tensor = torch.LongTensor(y_test)\n",
        "\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, X_train_tensor)\n",
        "val_dataset = TensorDataset(X_val_tensor, X_val_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, X_test_tensor)\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_dim, encoding_dim):\n",
        "        super(Autoencoder, self).__init__()\n",
        "\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(64, encoding_dim),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(encoding_dim, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(128, input_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "    def encode(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "\n",
        "model = Autoencoder(n_features, encoding_dim).to(device)\n",
        "print(model)\n",
        "\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=5, min_lr=1e-5)\n",
        "\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for data, _ in train_loader:\n",
        "        data = data.to(device)\n",
        "\n",
        "\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, data)\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * data.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    return epoch_loss\n",
        "\n",
        "\n",
        "def validate(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, _ in val_loader:\n",
        "            data = data.to(device)\n",
        "\n",
        "\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, data)\n",
        "\n",
        "            running_loss += loss.item() * data.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(val_loader.dataset)\n",
        "    return epoch_loss\n",
        "\n",
        "\n",
        "num_epochs = 100\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "best_val_loss = float('inf')\n",
        "patience = 20\n",
        "patience_counter = 0\n",
        "best_model_state = None\n",
        "\n",
        "print(\"Training Autoencoder...\")\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    val_loss = validate(model, val_loader, criterion, device)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}')\n",
        "\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        best_model_state = model.state_dict().copy()\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(f'Early stopping at epoch {epoch+1}')\n",
        "            break\n",
        "\n",
        "\n",
        "if best_model_state is not None:\n",
        "    model.load_state_dict(best_model_state)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.title('Autoencoder Training History')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "def extract_features(model, data_loader, device):\n",
        "    model.eval()\n",
        "    features = []\n",
        "    targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in data_loader:\n",
        "            data = data.to(device)\n",
        "            encoded_data = model.encode(data)\n",
        "            features.append(encoded_data.cpu().numpy())\n",
        "            targets.append(target.numpy())\n",
        "\n",
        "    return np.vstack(features), np.concatenate(targets)\n",
        "\n",
        "\n",
        "train_dataset_with_targets = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "val_dataset_with_targets = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "test_dataset_with_targets = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader_with_targets = DataLoader(train_dataset_with_targets, batch_size=batch_size)\n",
        "val_loader_with_targets = DataLoader(val_dataset_with_targets, batch_size=batch_size)\n",
        "test_loader_with_targets = DataLoader(test_dataset_with_targets, batch_size=batch_size)\n",
        "\n",
        "\n",
        "print(\"Extracting features...\")\n",
        "X_train_encoded, y_train_encoded = extract_features(model, train_loader_with_targets, device)\n",
        "X_val_encoded, y_val_encoded = extract_features(model, val_loader_with_targets, device)\n",
        "X_test_encoded, y_test_encoded = extract_features(model, test_loader_with_targets, device)\n",
        "\n",
        "print(f\"Original feature dimension: {X_train_scaled.shape[1]}\")\n",
        "print(f\"Encoded feature dimension: {X_train_encoded.shape[1]}\")\n",
        "\n",
        "\n",
        "np.save('X_train_encoded.npy', X_train_encoded)\n",
        "np.save('X_val_encoded.npy', X_val_encoded)\n",
        "np.save('X_test_encoded.npy', X_test_encoded)\n",
        "np.save('y_train_encoded.npy', y_train_encoded)\n",
        "np.save('y_val_encoded.npy', y_val_encoded)\n",
        "np.save('y_test_encoded.npy', y_test_encoded)\n",
        "\n",
        "\n",
        "torch.save(model.state_dict(), 'autoencoder_model.pth')\n",
        "torch.save(model.encoder.state_dict(), 'encoder_model.pth')\n",
        "\n",
        "print(\"Models and encoded features saved successfully.\")\n",
        "\n",
        "\n",
        "if encoding_dim >= 2:\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    if encoding_dim == 2:\n",
        "\n",
        "        plt.scatter(X_test_encoded[:, 0], X_test_encoded[:, 1], c=y_test_encoded, cmap='viridis', alpha=0.7)\n",
        "        plt.colorbar(label='CKD Stage')\n",
        "        plt.title('2D Encoded Features')\n",
        "        plt.xlabel('Feature 1')\n",
        "        plt.ylabel('Feature 2')\n",
        "    else:\n",
        "\n",
        "        from sklearn.manifold import TSNE\n",
        "        tsne = TSNE(n_components=2, random_state=42)\n",
        "        X_test_tsne = tsne.fit_transform(X_test_encoded)\n",
        "        plt.scatter(X_test_tsne[:, 0], X_test_tsne[:, 1], c=y_test_encoded, cmap='viridis', alpha=0.7)\n",
        "        plt.colorbar(label='CKD Stage')\n",
        "        plt.title('t-SNE Visualization of Encoded Features')\n",
        "        plt.xlabel('t-SNE Feature 1')\n",
        "        plt.ylabel('t-SNE Feature 2')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def analyze_encoded_features():\n",
        "    \"\"\"\n",
        "    Analyze the relationship between original features and encoded features\n",
        "    \"\"\"\n",
        "\n",
        "    encoder_weights = model.encoder[0].weight.data.cpu().numpy()\n",
        "\n",
        "\n",
        "    feature_importance = np.abs(encoder_weights).mean(axis=0)\n",
        "\n",
        "\n",
        "    importance_df = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Importance': feature_importance\n",
        "    }).sort_values('Importance', ascending=False)\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.barh(importance_df.head(20)['Feature'], importance_df.head(20)['Importance'])\n",
        "    plt.title('Top 20 Original Features by Importance to Encoded Representation')\n",
        "    plt.xlabel('Importance')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return importance_df\n",
        "\n",
        "\n",
        "importance_df = analyze_encoded_features()\n",
        "print(\"\\nTop 10 Important Original Features:\")\n",
        "print(importance_df.head(10))"
      ],
      "metadata": {
        "id": "EgtWKUQThCYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "X_train_encoded = np.load('X_train_encoded.npy')\n",
        "X_val_encoded = np.load('X_val_encoded.npy')\n",
        "X_test_encoded = np.load('X_test_encoded.npy')\n",
        "y_train_encoded = np.load('y_train_encoded.npy')\n",
        "y_val_encoded = np.load('y_val_encoded.npy')\n",
        "y_test_encoded = np.load('y_test_encoded.npy')\n",
        "\n",
        "print(f\"Training set: {X_train_encoded.shape[0]} samples\")\n",
        "print(f\"Validation set: {X_val_encoded.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test_encoded.shape[0]} samples\")\n",
        "print(f\"Input feature dimension: {X_train_encoded.shape[1]}\")\n",
        "print(f\"Number of classes: {len(np.unique(y_train_encoded))}\")\n",
        "\n",
        "print(\"Unique labels in training set:\", np.unique(y_train_encoded))\n",
        "print(\"Min label value:\", np.min(y_train_encoded))\n",
        "print(\"Max label value:\", np.max(y_train_encoded))\n",
        "\n",
        "if np.min(y_train_encoded) == 1:\n",
        "    print(\"Converting 1-indexed labels to 0-indexed...\")\n",
        "    y_train_encoded = y_train_encoded - 1\n",
        "    y_val_encoded = y_val_encoded - 1\n",
        "    y_test_encoded = y_test_encoded - 1\n",
        "    print(\"After conversion - Unique labels:\", np.unique(y_train_encoded))\n",
        "\n",
        "X_train_tensor = torch.FloatTensor(X_train_encoded)\n",
        "X_val_tensor = torch.FloatTensor(X_val_encoded)\n",
        "X_test_tensor = torch.FloatTensor(X_test_encoded)\n",
        "y_train_tensor = torch.LongTensor(y_train_encoded)\n",
        "y_val_tensor = torch.LongTensor(y_val_encoded)\n",
        "y_test_tensor = torch.LongTensor(y_test_encoded)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "class EnhancedDNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims=[64, 128, 64], output_dim=5, dropout_rate=0.25):\n",
        "        super(EnhancedDNN, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.input_layer = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dims[0]),\n",
        "            nn.BatchNorm1d(hidden_dims[0]),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        )\n",
        "\n",
        "        self.hidden_layers = nn.ModuleList()\n",
        "        for i in range(len(hidden_dims) - 1):\n",
        "            self.hidden_layers.append(nn.Sequential(\n",
        "                nn.Linear(hidden_dims[i], hidden_dims[i+1]),\n",
        "                nn.BatchNorm1d(hidden_dims[i+1]),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout_rate)\n",
        "            ))\n",
        "\n",
        "        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_layer(x)\n",
        "        for layer in self.hidden_layers:\n",
        "            x = layer(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "input_dim = X_train_encoded.shape[1]\n",
        "output_dim = len(np.unique(y_train_encoded))\n",
        "\n",
        "models_to_try = [\n",
        "    {\n",
        "        \"hidden_dims\": [32, 64, 32],\n",
        "        \"dropout_rate\": 0.2,\n",
        "        \"name\": \"Model 1 (Moderate)\"\n",
        "    },\n",
        "    {\n",
        "        \"hidden_dims\": [64, 128, 64],\n",
        "        \"dropout_rate\": 0.25,\n",
        "        \"name\": \"Model 2 (Wider)\"\n",
        "    },\n",
        "    {\n",
        "        \"hidden_dims\": [32, 64, 64, 32],\n",
        "        \"dropout_rate\": 0.3,\n",
        "        \"name\": \"Model 3 (Deeper)\"\n",
        "    }\n",
        "]\n",
        "\n",
        "best_val_acc = 0\n",
        "best_model = None\n",
        "best_model_name = \"\"\n",
        "\n",
        "def train_and_evaluate(model, model_name, train_loader, val_loader, num_epochs=100, patience=15):\n",
        "    print(f\"\\nTraining {model_name}...\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0005)\n",
        "\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='max', factor=0.5, patience=5, verbose=True\n",
        "    )\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "    best_val_acc_local = 0\n",
        "    patience_counter = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "        train_acc = 100 * correct / total\n",
        "\n",
        "\n",
        "        model.eval()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in val_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += targets.size(0)\n",
        "                correct += (predicted == targets).sum().item()\n",
        "\n",
        "        val_loss = running_loss / len(val_loader.dataset)\n",
        "        val_acc = 100 * correct / total\n",
        "\n",
        "        scheduler.step(val_acc)\n",
        "\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "\n",
        "\n",
        "        if val_acc > best_val_acc_local:\n",
        "            best_val_acc_local = val_acc\n",
        "            patience_counter = 0\n",
        "            best_model_state = model.state_dict().copy()\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f'Early stopping at epoch {epoch+1}')\n",
        "                break\n",
        "\n",
        "\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "\n",
        "    return model, best_val_acc_local, (train_losses, val_losses, train_accs, val_accs)\n",
        "\n",
        "for model_config in models_to_try:\n",
        "\n",
        "    model = EnhancedDNN(\n",
        "        input_dim=input_dim,\n",
        "        hidden_dims=model_config[\"hidden_dims\"],\n",
        "        output_dim=output_dim,\n",
        "        dropout_rate=model_config[\"dropout_rate\"]\n",
        "    ).to(device)\n",
        "\n",
        "\n",
        "    model, val_acc, history = train_and_evaluate(\n",
        "        model,\n",
        "        model_config[\"name\"],\n",
        "        train_loader,\n",
        "        val_loader\n",
        "    )\n",
        "\n",
        "    print(f\"{model_config['name']} - Best Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_model = model\n",
        "        best_model_name = model_config[\"name\"]\n",
        "        best_history = history\n",
        "\n",
        "print(f\"\\nBest model: {best_model_name} with validation accuracy: {best_val_acc:.2f}%\")\n",
        "\n",
        "model = best_model\n",
        "\n",
        "train_losses, val_losses, train_accs, val_accs = best_history\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.title(f'DNN Training Loss - {best_model_name}')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accs, label='Training Accuracy')\n",
        "plt.plot(val_accs, label='Validation Accuracy')"
      ],
      "metadata": {
        "id": "CS8LLC0ohQQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tabnet"
      ],
      "metadata": {
        "id": "dSjA7YtchTYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-tabnet"
      ],
      "metadata": {
        "id": "6gAdaM-xhWR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "X_train_np = X_train.values\n",
        "X_val_np = X_val.values\n",
        "X_test_np = X_test.values\n",
        "y_train_np = y_train.values\n",
        "y_val_np = y_val.values\n",
        "y_test_np = y_test.values\n",
        "\n",
        "\n",
        "all_targets = np.concatenate([y_train_np, y_val_np, y_test_np])\n",
        "unique_classes = np.unique(all_targets)\n",
        "class_mapping = {original: idx for idx, original in enumerate(sorted(unique_classes))}\n",
        "\n",
        "y_train_mapped = np.array([class_mapping[y] for y in y_train_np])\n",
        "y_val_mapped = np.array([class_mapping[y] for y in y_val_np])\n",
        "y_test_mapped = np.array([class_mapping[y] for y in y_test_np])\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_np = scaler.fit_transform(X_train_np)\n",
        "X_val_np = scaler.transform(X_val_np)\n",
        "X_test_np = scaler.transform(X_test_np)\n",
        "\n",
        "\n",
        "\n",
        "tabnet_model = TabNetClassifier(\n",
        "    optimizer_fn=torch.optim.Adam,\n",
        "    optimizer_params={'lr': 0.0005, 'weight_decay': 1e-4},\n",
        "    scheduler_params={\"step_size\": 10, \"gamma\": 0.9},\n",
        "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
        "    mask_type=\"entmax\",\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "tabnet_model.fit(\n",
        "    X_train=X_train_np, y_train=y_train_mapped,\n",
        "    eval_set=[(X_val_np, y_val_mapped)],\n",
        "    eval_name=['val'],\n",
        "    eval_metric=['accuracy'],\n",
        "    max_epochs=50,\n",
        "    patience=5,\n",
        "    batch_size=256,\n",
        "    virtual_batch_size=128,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "\n",
        "y_pred = tabnet_model.predict(X_test_np)\n",
        "test_accuracy = np.mean(y_pred == y_test_mapped)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "cm = confusion_matrix(y_test_mapped, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=unique_classes, yticklabels=unique_classes)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test_mapped, y_pred, target_names=[f'Stage {cls}' for cls in unique_classes]))\n",
        "\n",
        "\n",
        "\n",
        "def check_overfitting(train_losses, val_losses, train_accs, val_accs):\n",
        "    \"\"\"\n",
        "    Analyze training and validation metrics to detect overfitting.\n",
        "\n",
        "    Returns:\n",
        "        is_overfitting (bool): Whether the model shows signs of overfitting\n",
        "        analysis (dict): Detailed analysis metrics\n",
        "    \"\"\"\n",
        "    loss_gaps = [train - val for train, val in zip(train_losses, val_losses)]\n",
        "    acc_gaps = [train - val for train, val in zip(train_accs, val_accs)]\n",
        "\n",
        "    val_loss_increasing = False\n",
        "    for i in range(5, len(val_losses)):\n",
        "        if all(val_losses[i-j] < val_losses[i-j+1] for j in range(1, 5)) and train_losses[i] < train_losses[i-5]:\n",
        "            val_loss_increasing = True\n",
        "            break\n",
        "\n",
        "    significant_acc_gap = any(gap > 0.1 for gap in acc_gaps[-5:])\n",
        "\n",
        "    high_train_low_val_acc = train_accs[-1] > 0.95 and val_accs[-1] < 0.85\n",
        "\n",
        "    is_overfitting = val_loss_increasing or significant_acc_gap or high_train_low_val_acc\n",
        "\n",
        "    analysis = {\n",
        "        \"final_train_loss\": train_losses[-1],\n",
        "        \"final_val_loss\": val_losses[-1],\n",
        "        \"final_train_acc\": train_accs[-1],\n",
        "        \"final_val_acc\": val_accs[-1],\n",
        "        \"final_loss_gap\": loss_gaps[-1],\n",
        "        \"final_acc_gap\": acc_gaps[-1],\n",
        "        \"val_loss_increasing\": val_loss_increasing,\n",
        "        \"significant_acc_gap\": significant_acc_gap,\n",
        "        \"high_train_low_val_acc\": high_train_low_val_acc\n",
        "    }\n",
        "\n",
        "    return is_overfitting, analysis\n",
        "\n",
        "def visualize_overfitting(train_losses, val_losses, train_accs, val_accs):\n",
        "    \"\"\"\n",
        "    Create detailed visualizations to analyze overfitting.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "\n",
        "\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(train_accs, label='Training Accuracy')\n",
        "    plt.plot(val_accs, label='Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "\n",
        "    plt.subplot(2, 2, 3)\n",
        "    loss_gaps = [train - val for train, val in zip(train_losses, val_losses)]\n",
        "    plt.plot(loss_gaps)\n",
        "    plt.axhline(y=0, color='r', linestyle='--')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Train Loss - Val Loss')\n",
        "    plt.title('Loss Gap (negative values indicate underfitting)')\n",
        "\n",
        "\n",
        "    plt.subplot(2, 2, 4)\n",
        "    acc_gaps = [train - val for train, val in zip(train_accs, val_accs)]\n",
        "    plt.plot(acc_gaps)\n",
        "    plt.axhline(y=0, color='r', linestyle='--')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Train Acc - Val Acc')\n",
        "    plt.title('Accuracy Gap (large positive values indicate overfitting)')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "epochs = 50\n",
        "np.random.seed(42)\n",
        "\n",
        "train_losses = [0.8 - 0.015*i + np.random.normal(0, 0.01) for i in range(epochs)]\n",
        "val_losses = [0.8 - 0.01*i + np.random.normal(0, 0.01) for i in range(30)] + \\\n",
        "             [0.5 + 0.01*(i-30) + np.random.normal(0, 0.01) for i in range(30, epochs)]\n",
        "\n",
        "train_accs = [0.5 + 0.01*i + np.random.normal(0, 0.005) for i in range(epochs)]\n",
        "val_accs = [0.5 + 0.008*i + np.random.normal(0, 0.005) for i in range(30)] + \\\n",
        "           [0.74 + np.random.normal(0, 0.01) for i in range(30, epochs)]\n",
        "\n",
        "is_overfitting, analysis = check_overfitting(train_losses, val_losses, train_accs, val_accs)\n",
        "\n",
        "print(f\" model overfitting? {'Yes' if is_overfitting else 'No'}\")\n",
        "print(\"\\nDetailed Analysis:\")\n",
        "for key, value in analysis.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "# Visualize the metrics\n",
        "\n",
        "visualize_overfitting(train_losses, val_losses, train_accs, val_accs)\n",
        "\n"
      ],
      "metadata": {
        "id": "RDgOCz8whY9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "\n",
        "class TabularNet(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, hidden_dims, dropout_rate=0.3):\n",
        "        super(TabularNet, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "\n",
        "        for hidden_dim in hidden_dims:\n",
        "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
        "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
        "            layers.append(nn.LeakyReLU(0.1))\n",
        "            layers.append(nn.Dropout(dropout_rate))\n",
        "            prev_dim = hidden_dim\n",
        "\n",
        "        layers.append(nn.Linear(prev_dim, output_dim))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "def create_balanced_sampler(y_train):\n",
        "    class_counts = Counter(y_train)\n",
        "    weights = {class_idx: 1.0 / count for class_idx, count in class_counts.items()}\n",
        "    sample_weights = [weights[y] for y in y_train]\n",
        "    sampler = WeightedRandomSampler(sample_weights, len(sample_weights))\n",
        "    return sampler\n",
        "\n",
        "# New function to detect overfitting\n",
        "def detect_overfitting(train_losses, val_losses, train_accs, val_accs, epoch):\n",
        "    \"\"\"\n",
        "    Detect overfitting based on various metrics and provide recommendations.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing overfitting metrics and analysis\n",
        "    \"\"\"\n",
        "    # Initialize results dictionary\n",
        "    results = {\n",
        "        \"is_overfitting\": False,\n",
        "        \"severity\": \"None\",\n",
        "        \"metrics\": {},\n",
        "        \"recommendations\": []\n",
        "    }\n",
        "\n",
        "    # Need at least 5 epochs to detect trends\n",
        "    if epoch < 5:\n",
        "        return results\n",
        "\n",
        "    # Calculate metrics\n",
        "    current_train_loss = train_losses[-1]\n",
        "    current_val_loss = val_losses[-1]\n",
        "    loss_gap = current_train_loss - current_val_loss\n",
        "\n",
        "    current_train_acc = train_accs[-1]\n",
        "    current_val_acc = val_accs[-1]\n",
        "    acc_gap = current_train_acc - current_val_acc\n",
        "\n",
        "    # Check if validation loss is increasing while training loss is decreasing\n",
        "    val_loss_trend = np.polyfit(range(len(val_losses[-5:])), val_losses[-5:], 1)[0]\n",
        "    train_loss_trend = np.polyfit(range(len(train_losses[-5:])), train_losses[-5:], 1)[0]\n",
        "\n",
        "    # Store metrics\n",
        "    results[\"metrics\"] = {\n",
        "        \"train_loss\": current_train_loss,\n",
        "        \"val_loss\": current_val_loss,\n",
        "        \"loss_gap\": loss_gap,\n",
        "        \"train_acc\": current_train_acc,\n",
        "        \"val_acc\": current_val_acc,\n",
        "        \"acc_gap\": acc_gap,\n",
        "        \"val_loss_trend\": val_loss_trend,\n",
        "        \"train_loss_trend\": train_loss_trend\n",
        "    }\n",
        "\n",
        "    # Detect overfitting conditions\n",
        "    # 1. Validation loss increasing while training loss decreasing\n",
        "    diverging_loss = (val_loss_trend > 0 and train_loss_trend < 0)\n",
        "\n",
        "    # 2. Large accuracy gap\n",
        "    large_acc_gap = acc_gap > 0.05\n",
        "\n",
        "    # 3. Large negative loss gap (train loss much lower than val loss)\n",
        "    large_neg_loss_gap = loss_gap < -0.1\n",
        "\n",
        "    # Determine overfitting severity\n",
        "    if diverging_loss:\n",
        "        results[\"is_overfitting\"] = True\n",
        "        if large_neg_loss_gap and large_acc_gap:\n",
        "            results[\"severity\"] = \"Severe\"\n",
        "            results[\"recommendations\"] = [\n",
        "                \"Stop training immediately\",\n",
        "                \"Significantly increase regularization (dropout > 0.5)\",\n",
        "                \"Reduce model complexity\",\n",
        "                \"Add more training data or augmentation\"\n",
        "            ]\n",
        "        elif large_neg_loss_gap or large_acc_gap:\n",
        "            results[\"severity\"] = \"Moderate\"\n",
        "            results[\"recommendations\"] = [\n",
        "                \"Consider early stopping soon\",\n",
        "                \"Increase regularization (dropout by 0.1)\",\n",
        "                \"Try reducing learning rate\"\n",
        "            ]\n",
        "        else:\n",
        "            results[\"severity\"] = \"Mild\"\n",
        "            results[\"recommendations\"] = [\n",
        "                \"Monitor closely in next epochs\",\n",
        "                \"Consider slight increase in regularization\"\n",
        "            ]\n",
        "    elif large_neg_loss_gap:\n",
        "        results[\"is_overfitting\"] = True\n",
        "        results[\"severity\"] = \"Mild\"\n",
        "        results[\"recommendations\"] = [\n",
        "            \"Monitor closely in next epochs\",\n",
        "            \"Consider slight increase in regularization\"\n",
        "        ]\n",
        "\n",
        "    return results\n",
        "\n",
        "# New function to visualize training metrics and overfitting\n",
        "def plot_training_metrics(train_losses, val_losses, train_accs, val_accs, model_idx):\n",
        "    \"\"\"\n",
        "    Plot training and validation metrics to visualize overfitting.\n",
        "    \"\"\"\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Plot losses\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(epochs, train_losses, 'b-', label='Training Loss')\n",
        "    plt.plot(epochs, val_losses, 'r-', label='Validation Loss')\n",
        "    plt.title(f'Model {model_idx+1}: Training and Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot accuracies\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(epochs, train_accs, 'b-', label='Training Accuracy')\n",
        "    plt.plot(epochs, val_accs, 'r-', label='Validation Accuracy')\n",
        "    plt.title(f'Model {model_idx+1}: Training and Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot loss gap (overfitting indicator)\n",
        "    plt.subplot(2, 2, 3)\n",
        "    loss_gap = [t - v for t, v in zip(train_losses, val_losses)]\n",
        "    plt.plot(epochs, loss_gap, 'g-')\n",
        "    plt.axhline(y=0, color='k', linestyle='--')\n",
        "    plt.title(f'Model {model_idx+1}: Loss Gap (Train - Val)')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Gap')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Highlight overfitting regions\n",
        "    for i in range(1, len(val_losses)):\n",
        "        if i > 1 and val_losses[i] > val_losses[i-1] and train_losses[i] < train_losses[i-1]:\n",
        "            plt.axvspan(i, len(val_losses), alpha=0.2, color='yellow')\n",
        "            break\n",
        "\n",
        "    # Plot accuracy gap\n",
        "    plt.subplot(2, 2, 4)\n",
        "    acc_gap = [t - v for t, v in zip(train_accs, val_accs)]\n",
        "    plt.plot(epochs, acc_gap, 'g-')\n",
        "    plt.axhline(y=0, color='k', linestyle='--')\n",
        "    plt.title(f'Model {model_idx+1}: Accuracy Gap (Train - Val)')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Gap')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Final overfitting assessment\n",
        "    final_train_loss = train_losses[-1]\n",
        "    final_val_loss = val_losses[-1]\n",
        "    final_loss_gap = final_train_loss - final_val_loss\n",
        "    final_train_acc = train_accs[-1]\n",
        "    final_val_acc = val_accs[-1]\n",
        "    final_acc_gap = final_train_acc - final_val_acc\n",
        "\n",
        "    print(\"\\n===== OVERFITTING ASSESSMENT =====\")\n",
        "    print(f\"Final Training Loss: {final_train_loss:.4f}\")\n",
        "    print(f\"Final Validation Loss: {final_val_loss:.4f}\")\n",
        "    print(f\"Loss Gap (Train - Val): {final_loss_gap:.4f}\")\n",
        "    print(f\"Training Accuracy: {final_train_acc:.4f}\")\n",
        "    print(f\"Validation Accuracy: {final_val_acc:.4f}\")\n",
        "    print(f\"Accuracy Gap (Train - Val): {final_acc_gap:.4f}\")\n",
        "\n",
        "    # Determine if overfitting occurred\n",
        "    if final_loss_gap < -0.1 and any(val_losses[i] > val_losses[i-1] and train_losses[i] < train_losses[i-1] for i in range(1, len(val_losses))):\n",
        "        print(\"\\nVERDICT: Model is overfitting.\")\n",
        "        print(\"Evidence:\")\n",
        "        print(\"- Validation loss increased while training loss continued to decrease\")\n",
        "        print(f\"- Final loss gap is negative ({final_loss_gap:.4f})\")\n",
        "\n",
        "        # Find divergence point\n",
        "        for i in range(1, len(val_losses)):\n",
        "            if val_losses[i] > val_losses[i-1] and train_losses[i] < train_losses[i-1]:\n",
        "                print(f\"- Divergence began at epoch {i+1}\")\n",
        "                break\n",
        "    elif final_loss_gap < -0.05:\n",
        "        print(\"\\nVERDICT: Model shows signs of mild overfitting.\")\n",
        "        print(f\"- Loss gap is negative ({final_loss_gap:.4f})\")\n",
        "    else:\n",
        "        print(\"\\nVERDICT: Model shows minimal or no overfitting.\")\n",
        "\n",
        "def calculate_metrics(model, data_loader):\n",
        "    \"\"\"Calculate loss and accuracy for a given data loader\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in data_loader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    accuracy = correct / total\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def train_ensemble(X_train, y_train, X_val, y_val, X_test, y_test, class_mapping, unique_classes, n_models=5):\n",
        "    input_dim = X_train.shape[1]\n",
        "    output_dim = len(unique_classes)\n",
        "\n",
        "    X_train_tensor = torch.FloatTensor(X_train)\n",
        "    y_train_tensor = torch.LongTensor(y_train)\n",
        "    X_val_tensor = torch.FloatTensor(X_val)\n",
        "    y_val_tensor = torch.LongTensor(y_val)\n",
        "    X_test_tensor = torch.FloatTensor(X_test)\n",
        "\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "\n",
        "    sampler = create_balanced_sampler(y_train)\n",
        "\n",
        "    models = []\n",
        "    all_training_metrics = []\n",
        "\n",
        "    for model_idx in range(n_models):\n",
        "        print(f\"Training model {model_idx+1}/{n_models}\")\n",
        "\n",
        "        if model_idx == 0:\n",
        "            hidden_dims = [256, 128, 64]\n",
        "            dropout_rate = 0.3\n",
        "            batch_size = 64\n",
        "            lr = 0.001\n",
        "        elif model_idx == 1:\n",
        "            hidden_dims = [128, 128, 128]\n",
        "            dropout_rate = 0.4\n",
        "            batch_size = 128\n",
        "            lr = 0.0005\n",
        "        elif model_idx == 2:\n",
        "            hidden_dims = [512, 256, 128, 64]\n",
        "            dropout_rate = 0.25\n",
        "            batch_size = 32\n",
        "            lr = 0.002\n",
        "        elif model_idx == 3:\n",
        "            hidden_dims = [256, 256]\n",
        "            dropout_rate = 0.35\n",
        "            batch_size = 96\n",
        "            lr = 0.001\n",
        "        else:\n",
        "            hidden_dims = [384, 192, 96]\n",
        "            dropout_rate = 0.3\n",
        "            batch_size = 48\n",
        "            lr = 0.0015\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "        model = TabularNet(input_dim, output_dim, hidden_dims, dropout_rate)\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        patience = 15\n",
        "        patience_counter = 0\n",
        "        best_model_state = None\n",
        "\n",
        "        # Track metrics for overfitting detection\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        train_accs = []\n",
        "        val_accs = []\n",
        "\n",
        "        for epoch in range(100):\n",
        "            # Training phase\n",
        "            model.train()\n",
        "            train_loss = 0.0\n",
        "            train_correct = 0\n",
        "            train_total = 0\n",
        "\n",
        "            for inputs, targets in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                optimizer.step()\n",
        "\n",
        "                train_loss += loss.item()\n",
        "\n",
        "                # Calculate training accuracy\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                train_total += targets.size(0)\n",
        "                train_correct += (predicted == targets).sum().item()\n",
        "\n",
        "            train_loss /= len(train_loader)\n",
        "            train_acc = train_correct / train_total\n",
        "\n",
        "            # Validation phase\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            val_correct = 0\n",
        "            val_total = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for inputs, targets in val_loader:\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, targets)\n",
        "                    val_loss += loss.item()\n",
        "\n",
        "                    _, predicted = torch.max(outputs, 1)\n",
        "                    val_total += targets.size(0)\n",
        "                    val_correct += (predicted == targets).sum().item()\n",
        "\n",
        "            val_loss /= len(val_loader)\n",
        "            val_acc = val_correct / val_total\n",
        "\n",
        "            # Store metrics\n",
        "            train_losses.append(train_loss)\n",
        "            val_losses.append(val_loss)\n",
        "            train_accs.append(train_acc)\n",
        "            val_accs.append(val_acc)\n",
        "\n",
        "            # Check for overfitting\n",
        "            overfitting_results = detect_overfitting(train_losses, val_losses, train_accs, val_accs, epoch)\n",
        "\n",
        "            # Print progress and overfitting status\n",
        "            print(f\"Model {model_idx+1}, Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
        "                  f\"Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "            if overfitting_results[\"is_overfitting\"]:\n",
        "                print(f\"⚠️ Overfitting detected! Severity: {overfitting_results['severity']}\")\n",
        "                if overfitting_results[\"recommendations\"]:\n",
        "                    print(\"Recommendations:\")\n",
        "                    for rec in overfitting_results[\"recommendations\"]:\n",
        "                        print(f\"  - {rec}\")\n",
        "\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                best_model_state = model.state_dict().copy()\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= patience:\n",
        "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                    break\n",
        "\n",
        "        model.load_state_dict(best_model_state)\n",
        "        models.append(model)\n",
        "\n",
        "        # Store training metrics for this model\n",
        "        all_training_metrics.append({\n",
        "            'train_losses': train_losses,\n",
        "            'val_losses': val_losses,\n",
        "            'train_accs': train_accs,\n",
        "            'val_accs': val_accs\n",
        "        })\n",
        "\n",
        "        # Plot training metrics and overfitting analysis\n",
        "        plot_training_metrics(train_losses, val_losses, train_accs, val_accs, model_idx)\n",
        "\n",
        "    # Ensemble predictions\n",
        "    all_preds = []\n",
        "\n",
        "    for model in models:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = model(X_test_tensor)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.append(preds.numpy())\n",
        "\n",
        "    ensemble_preds = np.zeros_like(all_preds[0])\n",
        "    for i in range(len(X_test)):\n",
        "        votes = [preds[i] for preds in all_preds]\n",
        "        ensemble_preds[i] = max(set(votes), key=votes.count)\n",
        "\n",
        "    test_accuracy = accuracy_score(y_test, ensemble_preds)\n",
        "    print(f\"Ensemble Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "    # Plot ensemble confusion matrix\n",
        "    cm = confusion_matrix(y_test, ensemble_preds)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=unique_classes, yticklabels=unique_classes)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Ensemble Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Ensemble Classification Report:\")\n",
        "    print(classification_report(y_test, ensemble_preds, target_names=[f'Stage {cls}' for cls in unique_classes]))\n",
        "\n",
        "    # Analyze overfitting across all models in the ensemble\n",
        "    print(\"\\n===== ENSEMBLE OVERFITTING ANALYSIS =====\")\n",
        "    for model_idx, metrics in enumerate(all_training_metrics):\n",
        "        final_train_loss = metrics['train_losses'][-1]\n",
        "        final_val_loss = metrics['val_losses'][-1]\n",
        "        loss_gap = final_train_loss - final_val_loss\n",
        "\n",
        "        print(f\"Model {model_idx+1}:\")\n",
        "        print(f\"  Final Train/Val Loss: {final_train_loss:.4f}/{final_val_loss:.4f}\")\n",
        "        print(f\"  Loss Gap: {loss_gap:.4f}\")\n",
        "\n",
        "        if loss_gap < -0.1:\n",
        "            print(\"  Status: Overfitting ⚠️\")\n",
        "        elif loss_gap > 0.1:\n",
        "            print(\"  Status: Underfitting ⚠️\")\n",
        "        else:\n",
        "            print(\"  Status: Good fit ✓\")\n",
        "\n",
        "    return models, ensemble_preds, all_training_metrics\n",
        "\n",
        "# Function to analyze overfitting across the entire ensemble\n",
        "def analyze_ensemble_overfitting(all_training_metrics):\n",
        "    \"\"\"\n",
        "    Analyze overfitting patterns across all models in the ensemble\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Plot all models' validation losses\n",
        "    plt.subplot(2, 2, 1)\n",
        "    for i, metrics in enumerate(all_training_metrics):\n",
        "        plt.plot(metrics['val_losses'], label=f'Model {i+1}')\n",
        "    plt.title('Validation Loss Across All Models')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot all models' loss gaps\n",
        "    plt.subplot(2, 2, 2)\n",
        "    for i, metrics in enumerate(all_training_metrics):\n",
        "        loss_gaps = [t - v for t, v in zip(metrics['train_losses'], metrics['val_losses'])]\n",
        "        plt.plot(loss_gaps, label=f'Model {i+1}')\n",
        "    plt.axhline(y=0, color='k', linestyle='--')\n",
        "    plt.title('Loss Gap (Train - Val) Across All Models')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Gap')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot final loss gaps as bar chart\n",
        "    plt.subplot(2, 2, 3)\n",
        "    final_loss_gaps = []\n",
        "    for metrics in all_training_metrics:\n",
        "        final_loss_gaps.append(metrics['train_losses'][-1] - metrics['val_losses'][-1])\n",
        "\n",
        "    bars = plt.bar(range(1, len(final_loss_gaps) + 1), final_loss_gaps)\n",
        "    plt.axhline(y=0, color='k', linestyle='--')\n",
        "    plt.title('Final Loss Gap by Model')\n",
        "    plt.xlabel('Model')\n",
        "    plt.ylabel('Final Loss Gap')\n",
        "\n",
        "    # Color bars based on overfitting/underfitting\n",
        "    for i, bar in enumerate(bars):\n",
        "        if final_loss_gaps[i] < -0.1:\n",
        "            bar.set_color('red')  # Overfitting\n",
        "        elif final_loss_gaps[i] > 0.1:\n",
        "            bar.set_color('orange')  # Underfitting\n",
        "        else:\n",
        "            bar.set_color('green')  # Good fit\n",
        "\n",
        "    # Add a legend\n",
        "    from matplotlib.patches import Patch\n",
        "    legend_elements = [\n",
        "        Patch(facecolor='red', label='Overfitting'),\n",
        "        Patch(facecolor='green', label='Good fit'),\n",
        "        Patch(facecolor='orange', label='Underfitting')\n",
        "    ]\n",
        "    plt.legend(handles=legend_elements)\n",
        "\n",
        "    # Plot final train vs val accuracy\n",
        "    plt.subplot(2, 2, 4)\n",
        "    final_train_accs = [metrics['train_accs'][-1] for metrics in all_training_metrics]\n",
        "    final_val_accs = [metrics['val_accs'][-1] for metrics in all_training_metrics]\n",
        "\n",
        "    x = np.arange(len(final_train_accs))\n",
        "    width = 0.35\n",
        "\n",
        "    plt.bar(x - width/2, final_train_accs, width, label='Train Accuracy')\n",
        "    plt.bar(x + width/2, final_val_accs, width, label='Val Accuracy')\n",
        "\n",
        "    plt.title('Final Train vs Val Accuracy by Model')\n",
        "    plt.xlabel('Model')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xticks(x, [f'Model {i+1}' for i in range(len(final_train_accs))])\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "models, ensemble_preds, all_metrics = train_ensemble(X_train_np, y_train_mapped, X_val_np, y_val_mapped, X_test_np, y_test_mapped, class_mapping, unique_classes)\n",
        "analyze_ensemble_overfitting(all_metrics)"
      ],
      "metadata": {
        "id": "kicEzpzThs2i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}